# Databricks notebook source
# MAGIC %md
# MAGIC ## Purpose
# MAGIC
# MAGIC This notebook analyzes the duplicate candidate data generated by Notebook 1 and saves the results to a table
# MAGIC with column descriptions. This table provides insights into the distribution of similarity scores, helping users
# MAGIC select optimal threshold values for deduplication.
# MAGIC
# MAGIC ## Parameters
# MAGIC
# MAGIC `Analysis Table Name` (widget): The name of the table to save the analysis results to.
# MAGIC
# MAGIC ## Outputs
# MAGIC
# MAGIC Creates a table with the specified name containing the duplicate candidate analysis results.

# COMMAND ----------

# MAGIC %md
# MAGIC #### Notebook Setup

# COMMAND ----------

dbutils.widgets.text("catalog_name", "", "Catalog Name")
catalog_name = dbutils.widgets.get("catalog_name")
if not catalog_name:
    raise Exception("Catalog name is required to run this notebook")
analysis_table_name = f"{catalog_name}.analysis.duplicate_analysis"

# COMMAND ----------

# MAGIC %md
# MAGIC #### Threshold Definition
# MAGIC
# MAGIC As part of this notebook's output will be a display of total number of candidates relative to the
# MAGIC threshold. We will iterate through the thresholds to determine the right number to get the right quality,
# MAGIC and this section defines those thresholds.

# COMMAND ----------

# Define the thresholds to analyze
thresholds = [0.99, 0.98, 0.97, 0.96, 0.95, 0.94, 0.93, 0.92, 0.91, 0.90, 0.85, 0.80, 0.75]

# COMMAND ----------

# MAGIC %md
# MAGIC #### Entity Definition
# MAGIC
# MAGIC List of entities for processing. You must define:
# MAGIC
# MAGIC *   `table_name`: fully qualified table name with catalog and schema (3-part naming).
# MAGIC *   `primary_key`: column that uniquely identifies the column. Used for model operations.
# MAGIC *   `columns_to_exclude`: columns that aren't suitable for model vector generation that can disrupt
# MAGIC       overall quality.

# COMMAND ----------

# Define the entities to process (same as Notebook 1)
entities = [
    {"table_name": f"{catalog_name}.bronze.provider", "primary_key": "provider_id", "columns_to_exclude": ["provider_id"]},
    {"table_name": f"{catalog_name}.bronze.speciality", "primary_key": "speciality_id", "columns_to_exclude": ["speciality_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.license_and_credential", "primary_key": "license_credential_id", "columns_to_exclude": ["license_credential_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.affiliations", "primary_key": "affiliation_id", "columns_to_exclude": ["affiliation_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.address_and_location", "primary_key": "address_location_id", "columns_to_exclude": ["address_location_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.contact_information", "primary_key": "contact_id", "columns_to_exclude": ["contact_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.ProviderNetworkParticipation", "primary_key": "network_id", "columns_to_exclude": ["network_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.Employment_and_Contracts", "primary_key": "employment_contract_id", "columns_to_exclude": ["employment_contract_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.Education_and_Training", "primary_key": "education_training_id", "columns_to_exclude": ["education_training_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.Performance_and_QualityMetrics", "primary_key": "performance_metric_id", "columns_to_exclude": ["performance_metric_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.Identifiers", "primary_key": "identifier_id", "columns_to_exclude": ["identifier_id", "provider_id"]},
    {"table_name": f"{catalog_name}.bronze.Digital_Presence", "primary_key": "digital_presence_id", "columns_to_exclude": ["digital_presence_id", "provider_id"]}
]

# COMMAND ----------

# MAGIC %md
# MAGIC #### Column Schema

# COMMAND ----------

from pyspark.sql.functions import lit, col, expr, percentile_approx
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType

# Define the schema for the analysis table
analysis_schema = StructType([
    StructField("entity_name", StringType(), True),
    StructField("threshold", DoubleType(), True),
    StructField("duplicate_count", LongType(), True),
    StructField("percent_duplicates", DoubleType(), True),
    StructField("total_record_count", LongType(), True),
    StructField("avg_search_score", DoubleType(), True),
    StructField("median_search_score", DoubleType(), True),
    StructField("potential_duplicate_pairs_above_99", LongType(), True),
    StructField("top_quartile_score", DoubleType(), True),
    StructField("records_with_max_score", LongType(), True)
])

# COMMAND ----------

# MAGIC %md
# MAGIC #### Creating the `analyze_entity` function
# MAGIC
# MAGIC This function will create the data model to be saved into our table. It calculates the duplicates, percentages,
# MAGIC averages, and other key data that we want to save and analyze for making a decision about thresholds and
# MAGIC models.

# COMMAND ----------

# Function to generate the analysis data for a single entity
def analyze_entity(entity, thresholds):
    table_name = entity["table_name"]
    entity_name = table_name.split(".")[-1]
    candidate_view = f"{table_name}_duplicate_candidates"

    # Get total record count for the entity
    total_record_count = spark.table(table_name).count()

    # Function to calculate the duplicate count for a given threshold
    def calculate_duplicate_count(threshold):
        duplicate_count_df = spark.sql(f"""
            SELECT COUNT(DISTINCT original_id) AS duplicate_count
            FROM {candidate_view}
            WHERE search_score >= {threshold}
        """)
        return duplicate_count_df.collect()[0]["duplicate_count"]

    # Function to calculate percent duplicate of total record
    def calculate_percentage_duplicates(duplicate_count, total_record_count):
        return (duplicate_count / total_record_count) * 100

    # Calculate average search score
    avg_search_score_df = spark.sql(f"""
        SELECT AVG(search_score) AS avg_search_score
        FROM {candidate_view}
    """)
    avg_search_score = avg_search_score_df.collect()[0]["avg_search_score"]

    # Calculate median search score
    median_search_score_df = spark.sql(f"""
        SELECT percentile_approx(search_score, 0.5) AS median_search_score
        FROM {candidate_view}
    """)
    median_search_score = median_search_score_df.collect()[0]["median_search_score"]

    # Calculate the number of duplicate pairs above .99
    potential_duplicate_pairs_above_99_df = spark.sql(f"""
        SELECT SUM(CASE WHEN search_score > 0.99 THEN 1 ELSE 0 END) as potential_duplicate_pairs_above_99
        FROM {candidate_view}
    """)
    potential_duplicate_pairs_above_99 = potential_duplicate_pairs_above_99_df.collect()[0]["potential_duplicate_pairs_above_99"]

    # Calculate top quartile search score
    top_quartile_score_df = spark.sql(f"""
        SELECT percentile_approx(search_score, 0.75) AS top_quartile_score
        FROM {candidate_view}
    """)
    top_quartile_score = top_quartile_score_df.collect()[0]["top_quartile_score"]

    # Calculate the number of records with max score
    records_with_max_score_df = spark.sql(f"""
        SELECT COUNT(DISTINCT original_id) AS records_with_max_score
        FROM (
            SELECT original_id, MAX(search_score) AS max_score
            FROM {candidate_view}
            GROUP BY original_id
        ) t
        WHERE max_score = 1.0
    """)
    records_with_max_score = records_with_max_score_df.collect()[0]["records_with_max_score"]

    # Create a list of rows for each threshold
    analysis_data = []
    for threshold in thresholds:
        duplicate_count = calculate_duplicate_count(threshold)
        percentage_duplicate = calculate_percentage_duplicates(duplicate_count, total_record_count)

        analysis_data.append((
            entity_name,
            threshold,
            duplicate_count,
            percentage_duplicate,
            total_record_count,
            avg_search_score,
            median_search_score,
            potential_duplicate_pairs_above_99,
            top_quartile_score,
            records_with_max_score
        ))

    return analysis_data

# COMMAND ----------

# MAGIC %md
# MAGIC #### Main Execution Loop
# MAGIC
# MAGIC For each entity in our model, we are executing our analysis process on.

# COMMAND ----------

# Main execution loop
analysis_data = []
for entity in entities:
    table_name = entity["table_name"]
    entity_name = table_name.split(".")[-1]
    candidate_view = f"{table_name}_duplicate_candidates"
    # Ensure the duplicate candidate view exists
    try:
        spark.table(candidate_view)
    except Exception as e:
        print(f"Skipping entity {table_name} because duplicate candidate view '{candidate_view}' was not found.")
        continue

    print(f"Analyzing entity: {table_name}")
    analysis_data.extend(analyze_entity(entity, thresholds))

# COMMAND ----------

# MAGIC %md
# MAGIC #### Create Data Model

# COMMAND ----------

# Create the analysis DataFrame
analysis_df = spark.createDataFrame(analysis_data, schema=analysis_schema)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Save Results

# COMMAND ----------

# Write the analysis data to a table
analysis_df.write.mode("overwrite").saveAsTable(analysis_table_name)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Add Column Descriptions
# MAGIC
# MAGIC The column descriptions will assist you in further understanding your data, the quality of the model, and the
# MAGIC proper threshold. You may need to modify this in order to correctly specify your keys, and other model
# MAGIC parameters. These definitions can be found below.

# COMMAND ----------

# Add column descriptions to the analysis table
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `entity_name` COMMENT 'The name of the entity (table) being analyzed (e.g., "provider", "speciality"). Use this to identify which data set the data is derived from';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `threshold` COMMENT 'A predefined similarity score threshold used for analysis. Higher values indicate a stricter match. Analyze with duplicate counts to evaluate the sensitivity of the model. Recommended use: if values are too low, it is recommended the vector index be tuned, thresholds be more strictly defined, and the model retrained';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `duplicate_count` COMMENT 'The number of potential duplicate records found in the entity at or above the given threshold. A higher count suggests more potential duplicates. Use this with total_record_count and threshold to determine whether the model has an acceptable number of duplicates, based on the model sensitivity';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `percent_duplicates` COMMENT 'The percentage of records in the entity that have at least one potential duplicate at or above the given threshold. A high percentage suggests a large proportion of duplicate data. Combine this info to understand the number of records vs model sensitivity (duplicate_count, number of records above 0.99, # of top duplicates';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `total_record_count` COMMENT 'The total number of records in the entity (from the original bronze table). Use this to calculate the percentage of duplicates.';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `avg_search_score` COMMENT 'The average search score for all potential duplicate pairs in the entity. Provides a general sense of the overall similarity of the data. Combine this with thresholds, max score, and # of duplicates to ensure the model is working correctly';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `median_search_score` COMMENT 'The median search score for all potential duplicate pairs in the entity. This is often a more robust measure of central tendency than the average, as it is less sensitive to outliers. Use the value and variance for median to evaluate model correctness';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `potential_duplicate_pairs_above_99` COMMENT 'The number of potential duplicate pairs with a similarity score above 0.99. A high value may suggest that the data is highly similar or that the similarity search is too aggressive. Use with thresholds, max score, and # of duplicates to ensure the model is working correctly';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `top_quartile_score` COMMENT 'The 75th percentile search score for a given entity. This indicates the score above which 25% of the duplicate pairs fall. Use the value and variance for the value to evaluate model correctness';
""")
spark.sql(f"""
ALTER TABLE {analysis_table_name}
ALTER COLUMN `records_with_max_score` COMMENT 'The number of original records where the highest search score among its potential duplicates is 1.0. A high value may indicate that the num_results parameter in Notebook 1 is too low and you may wish to revisit to increase number of potential results coming back. It may also indicate that the data has perfect duplicates. If the counts are small, then consider moving to notebook number 2 to start filtering out duplicates';
""")
